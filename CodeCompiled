#importing packages & libraries
import pandas as pd
from pandas import *
import requests
from bs4 import BeautifulSoup
import time



# EXCEL IMPORT & EXPORT 

# imports data from excel based on location, drops unneeded columns
file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'
iPortal_import = pd.read_excel(file_path + 'iPortal.xlsx', sheet_name = 'Assets')
iPortal_import = iPortal_import.drop(columns = ['Type', 'PLC', 'Rack', 'SubAddress', 'Slot', 'SubSlot', 'Module', 'Order number',
                                           'GSDML files', 'Online-MLFB', 'Online-HW version', 'Online serial No.', 'IP',
                                           'Subnet', 'Gateway', 'MAC', 'Device ID', 'Robot', 'Rob. Cell name', 'Rob. Controller', 
                                            'Rob. Serial.', 'Rob. KSS', 'ANZ. Ext. Axes', 'PN-stack active', 'ANZ. secure IOs', 'Number of IOs',
                                           'Version of PROFINET', 'MasterBusCycleTime', 'MasterBusTimeout', 'SlaveBusCycleTime', 
                                           'SlaveBusTimeout', 'Profienergy enabled', 'ProfiEnergyTimeToPauseHibernate',
                                           'ProfiEnergyTimeMinLengthOfStayHibernate', 'ProfiEnergyTimeToOperateHibernate',
                                           'ProfiEnergyTimeToPauseDrivesOff', 'ProfiEnergyTimeMinLengthOfStayDrivesOff',
                                           'ProfiEnergyTimeToOperateDrivesOff', 'PN product name', 'Active participants',
                                            'User-ID','Cycle time', 'SubSystem', 'Version', 'comment', 'Manufacturer'])
print(iPortal_import)

# filling in empty/NaN cells w/ value preceding the cell 
iPortal_data = iPortal_import.fillna(method='ffill')
print(iPortal_data)

# list of the names of all products
productList = iPortal_data['Product name'].unique()
pList = pd.DataFrame(productList)
print(pList)

# number of unique products 
len(productList)

# data frame that house the count of each device/product 
productCount = iPortal_data['Product name'].value_counts()




##########################          SIEMENS SCRAPE       ######################


#request to get html from website
r = requests.get('https://cert-portal.siemens.com/productcert/rss/advisories.atom')
soup = BeautifulSoup(r.text)


vulns = soup.find_all('entry')
data = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}
master_df = pd.DataFrame(data=data)


##function to get info from each vuln
def scrape(vulns):
    url = str(vulns).split('<id>', 1)[1]
    url = url.split('</id>')[0]
    url = url.replace('pdf', 'txt', 2)
    print(url)
    r = requests.get(url)
    soup = BeautifulSoup(r.text)
    html = str(soup).lower()
    
    df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}
    df = pd.DataFrame(data=df)

    #setting values
    
    #name
    name = 'siemens'
    
    #link 
    link = url
    
    #cvss score
    if('base score:') in html:
        cvss = html.split('base score:', 1)[1]
        cvss = cvss.split('summary')[0]
        cvss = cvss.strip()
    else:
        cvss = 'NULL'
    
    #vector
    if('cvss vector') in html:
        vector = html.split('cvss vector', 1)[1]
        vector = vector.split('/', 1)[1]
        vector = vector.split('cwe')[0]
        vector = vector.strip()
    else:
        vector = 'NULL'
    
    #products
    if('affected products and solution') in html:
        products = html.split('affected products and solution', 1)[1]
        products = products.split('==============================', 1)[1]
        products = products.split('workarounds')[0]
        products = products.strip()
    else:
        products = 'NULL'
    
    data = {'Vendor': [name], 'CVSS Score': [cvss], 'Vector': [vector], 'Products': [products], 'Link': [link]}
    temp_df = pd.DataFrame(data=data)
    df = df.append(temp_df)
    df = df[df.Vendor != '0']
    
    return(df)

for i in range(len(vulns)):
    temp_df = scrape(vulns[i])
    master_df = master_df.append(temp_df)
    

siemens_df = master_df 





#########################           SICK AG SCRAPE          ###########################


#request to get html from website
r = requests.get('https://www.sick.com/de/en/service-and-support/the-sick-product-security-incident-response-team-sick-psirt/w/psirt/#advisories & https://www.us-cert.gov/ics/advisories-by-vendor-last-revised-date?page=9')
soup = BeautifulSoup(r.text)


#gets the object that contians information on vulnerabilities
advisories = soup.find('tbody')

#essentailly breaks up all of the vulnerabilities into their rows and contains them in a list
items = advisories.find_all('tr')

#initialize dataframe
d = {'Vendor': ['0'], 'CVSS Score': ['0'],'Vector': ['0'], 'Products': ['0'], 'PDF': ['0']}
sick_vuln_df = pd.DataFrame(data=d)
sick_vuln_df

#for loop to add info from rows to dataframe, iterate through items
length = len(items)
index = 1
for i in range(length-1):
    
    #creating variables based on indexed information on the vulnerability
    work = items[index]
    tds = work.find_all('td')
    vendor = 'Sick AG'
    CVSS = tds[2].get_text()
    Products = tds[3].get_text()
    
    #PDF url needs to be converted into a string and made into a substring and then appended to base url
    PDF = str(tds[5])
    string = str(PDF)
    update = string.split('href="', 1)[1]
    final = update.split('"')[0]
    url = 'https://www.sick.com' + final
    
    #creating data set to create temp dataframe to append to master dataframe of vulnerabilities for SICK
    d = {'Vendor': [vendor], 'CVSS Score': [CVSS], 'Vector': 'NULL', 'Products': [Products], 'PDF': [url]}
    temp_df = pd.DataFrame(data=d)
    sick_vuln_df = sick_vuln_df.append(temp_df)
    index = index + 1
        
sick_vuln_df = sick_vuln_df.reset_index()
sick_vuln_df = sick_vuln_df.drop(columns = ['index'])
sick_vuln_df = sick_vuln_df.drop([0])



##############################               US CERT SCRAPE           #######################

#request to get html from website
r = requests.get('https://www.us-cert.gov/ics/advisories-by-vendor-last-revised-date?page=0')
soup = BeautifulSoup(r.text)

#function that gets a pages html and then returns a list of all vendors html on the page
def vendorsPerPage(url):
    r = requests.get(url)
    soup = BeautifulSoup(r.text)
    
    #getting a list of all vendors html on the page
    body = soup.find(role='main')
    div = body.find(lambda tag: tag.name == 'div' and tag['class'] == ['view-content'])
    item_list = div.find_all('div')
    return(item_list)



#function that pulls all links of a vendor
def vendorLinks(vendor_html):
    link_list = vendor_html.find_all('a', href=True)
    for i in range(len(link_list)):
        update = str(link_list[i]).split('href="', 1)[1]
        final = update.split('"')[0]
        link_list[i] = final
    return(link_list)



#function that scrapes data from link list and creates a df for the vendor
def vulnScrape(link_list, name):
    print(name)
    print(link_list)
    print(len(link_list))
    baseUrl = 'https://www.us-cert.gov'
    df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}
    df = pd.DataFrame(data=df)
    if len(link_list) >1:
        for i in range((len(link_list))):
            url = baseUrl + link_list[i]
            vuln_df = vulnInfo(url, name)
            df = df.append(vuln_df)
            
    else:
        print(link_list)
        link_list = str(link_list).split("['", 1)[1]
        link_list = link_list.split("']")[0]
        url = baseUrl + str(link_list)
        vuln_df = vulnInfo(url, name)
        df = df.append(vuln_df)

    df = df[df.Vendor != '0']        
    return(df)



#function that goes to vuln page and scrapes inf0
df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}
df = pd.DataFrame(data=df)

def vulnInfo(url, name):
    print(url)
    try:
        r = requests.get(url)
        soup = BeautifulSoup(r.text)
    except:
        time.sleep(2)
        r = requests.get(url)
        soup = BeautifulSoup(r.text)
        
    vendor = name
    
    #parsing cvss score from html
    cvss = str(soup)
    cvss = cvss.lower()
    if('a cvss') in cvss:
        cvss = cvss.split('a cvss', 1)[1]
        if('score of') in cvss:
            cvss = cvss.split('score of ', 1)[1]
            cvss = cvss.split(' ')[0]
            cvss = cvss[0:3]
        else:
            cvss = cvss.split('score: ', 1)[1]
            cvss = cvss.split(' ')[0]
            cvss = cvss[0:3]
    else:
        cvss = 'NULL'
    
    #parsing vector from html
    vector = str(soup)
    vector = vector.lower()
    if('cvss vector') in vector:
        if('cvss vector string is') in vector:
            vector = vector.split('cvss vector string is', 1)[1]
            if ((vector.index('(')) == 0) and ((vector.index('a') == 1)):
                vector = vector.split('.')[0]
            elif ('(cvss:') in vector:
                vector = vector.split('/', 1)[1]
                vector = vector.split('.')[0]
                vector = '(' + vector
            elif('>a') in vector:
                vector = vector.split('/a', 1)[1]
                vector = vector.split('"')[0]
                vector = 'a' + vector
                if(')') in vector:
                    vector = vector.split(')')[0]
            else:
                vector = vector.split('>', 1)[1]
                vector = vector.split('<')[0]
        else:
            vector = vector.split('cvss vector string', 1)[1]
            vector = vector.split('is', 1)[1]
            if ((vector.index('(')) == 0) and ((vector.index('a') == 1)):
                vector = vector.split('.')[0]
            elif ('(cvss:') in vector:
                vector = vector.split('/', 1)[1]
                vector = vector.split('.')[0]
                vector = '(' + vector
            elif('>a') in vector:
                vector = vector.split('/a', 1)[1]
                vector = vector.split('"')[0]
                vector = 'a' + vector
                if(')') in vector:
                    vector = vector.split(')')[0]
            else:
                vector = vector.split('>', 1)[1]
                vector = vector.split('<')[0]
    else:
        vector = 'NULL'
    
    #parsing products affected from html
    products = str(soup)
    products = products.lower()
    if('affected products') in products:
        if('affected:') in products:
            print('1')
            products = products.split('affected:', 1)[1]
            products = products.split('</p>', 1)[1]
            if (products.index('u')) == 0:
                print('1a')
                products = products.split('<li>', 1)[1]
                products = products.split('</li>')[0]
            elif (products.index('p')) == 0:
                print('1b')
                print(products)
                if('<li>') in products:
                    products = products.split('<li>', 1)[1]
                    products = products.split('</li>')[0]
                elif('<p>') in products:
                    products = products.split('<p>', 1)[1]
                    products = products.split('</p')[0]
            else:
                if('<ul>') in products:
                    products = products.split('<ul>', 1)[1]
                    products = products.split('</ul>')[0]
                elif('<p>') in products:
                    products = products.split('<p>', 1)[1]
                    products = products.split('</p')[0]
        elif('affected products') in products:
            print('2')
            products = products.split('affected products', 1)[1]
            products = products.split('/h3', 1)[1]
            products = products.split('<h3>')[0]
        else:
            print('3')
            products = products.split('<p>', 1)[1]
            products = products.split('</p>')[0]
    else:
        products = 'NULL'
   
    #appending variables to df
    vuln = {'Vendor': [name], 'CVSS Score': [cvss], 'Vector': [vector], 'Products': [products], 'Link': [url]}
    vuln_df = pd.DataFrame(data=vuln)
    
    #dropping row in df with 0 values
    vuln_df = vuln_df.reset_index(drop=True)
    return(vuln_df)


#iterating through all pages of the website

#getting down to the code that holds the url for the last page of results
basePgUrl = 'https://www.us-cert.gov/ics/advisories-by-vendor-last-revised-date?page='
pg = soup.find_all(role = 'navigation')
buttons = pg[1].find_all('li')
string = str(buttons[10])
update = string.split('href="?page=', 1)[1]
final = update.split('"')[0]
numbOfPgs = int(final)
df = {'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'], 'Products': ['0'], 'Link': ['0']}
master_df = pd.DataFrame(data=df)

for i in range(numbOfPgs+1):
    pgUrl = basePgUrl + str(i) 
    item_list = vendorsPerPage(pgUrl)
    length = len(item_list)
    for x in range(length):
        print(x)
        vName = item_list[x].find('h3')
        name = vName.get_text()
        name = name.lower() 
        link_list = vendorLinks(item_list[x])
        df = vulnScrape(link_list, name)
        master_df = master_df.append(df)
        


us_cert_df = master_df



######################            APPENDING ALL DFS TOGETHER            ################


#appending all dfs together

master_df = siemens_df
master_df = master_df.append(sick_vuln_df)
master_df = master_df.append(us_cert_df)



###################                 EXPORTING TO EXCEL          #############

# export data to new excel
writer = pd.ExcelWriter(file_path + 'allData.xlsx')
iPortal_data.to_excel(writer, 'IPortal')
pList.to_excel(writer, 'List of Products')
productCount.to_excel(writer, 'Count per Product')

siemens_df = siemens_df[siemens_df.Vendor != '0']
siemens_df.to_excel(writer, 'siemensScrape')

sick_vuln_df = sick_vuln_df[sick_vuln_df.Vendor != '0']
sick_vuln_df.to_excel(writer, 'sickAGScrape')

us_cert_df.to_excel(writer, 'US_CertScrape')

master_df.to_excel(writer, 'MasterFile')
writer.save()


######################################             CLEANING AND MERGING DFS              ##############################

#############          IMPORTING FROM EXCEL          ##############


#importing vulnerability data from excel sheets

file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'

#SICK Data
Sick_Import = pd.read_excel(file_path + 'SickData.xlsx', sheet_name = 'Scrape_Data')
Sick_Import = Sick_Import.reset_index()
Sick_Import = Sick_Import.drop(columns = ['index'])


#Siemens Data
Siemens_Import = pd.read_excel(file_path + 'SiemensData.xlsx', sheet_name = 'Scrape_Data')
Siemens_Import = Siemens_Import.reset_index()
Siemens_Import = Siemens_Import.drop(columns = ['index'])


#US Cert Data
UsCert_Import = pd.read_excel(file_path + 'UsCertData.xlsx', sheet_name = 'Scrape_Data')
UsCert_Import = UsCert_Import.reset_index()
UsCert_Import = UsCert_Import.drop(columns = ['index'])

#getting vulnerabilities for different vendors from US CERT scrape

#gettings Siemens vulnerabilties 
fSiemens = UsCert_Import[UsCert_Import['Vendor'].str.contains('siemens')]
fSiemens['Vendor'] = 'siemens'
fSiemens = fSiemens.reset_index()
fSiemens = fSiemens.drop(columns = ['index'])


#getting Pepperl Fuchs vulnerabilities from US Cert
fPepperl = UsCert_Import[UsCert_Import['Vendor'].str.contains('pepperl')]
fPepperl['Vendor'] = 'pepperl fuchs'
fPepperl = fPepperl.reset_index()
PepperlVuln = fPepperl.drop(columns = ['index'])

#getting Pepperl Fuchs vulnerabilities from pepperl scrape
Pepperl_Import = pd.read_excel(file_path + 'PepperlData.xlsx', sheet_name = 'Sheet1')
Pepperl_Import = Pepperl_Import.reset_index()
Pepperl_Import = Pepperl_Import.drop(columns = ['index'])
Pepperl_Import['Vendor'] = 'pepperl fuchs'
Pepperl_Import = Pepperl_Import.apply(lambda x: x.astype(str).str.lower())
Pepperl_Import = Pepperl_Import.drop(columns = ['Unnamed: 4', 'Products'])
Pepperl_Import = Pepperl_Import.rename(columns = {'Links': 'Link'})

#merging two pepperl vulnerability dataframes together
PepperlVuln = PepperlVuln.rename(columns = {'Products': 'Description'})
PepperlVuln = PepperlVuln.append(Pepperl_Import)
PepperlVuln = PepperlVuln.rename(columns = {'Description': 'Products'})


#getting sick vulnerabilities
fSick = UsCert_Import[UsCert_Import['Vendor'].str.contains('sick')]
fSick = fSick.reset_index()
fSick = fSick.drop(columns = ['index'])


#getting festo vulnerabilities
fFesto = UsCert_Import[UsCert_Import['Vendor'].str.contains('fest')]
fFesto = fFesto.reset_index()
fFesto = fFesto.drop(columns = ['index'])




#finalized vulnerabilities dfs

#sick
SickDf = pd.DataFrame(data=Sick_Import)
SickDf = SickDf.rename(columns = {'PDF': 'Link'})
SickDf = SickDf.append(fSick)
SickDf['Vendor'] = 'sick ag'
SickVuln = SickDf
SickVuln = SickVuln.drop_duplicates()
SickVuln = SickVuln.reset_index()
SickVuln = SickVuln.drop(columns = 'index')

#siemens
SiemensVuln = fSiemens.append(Siemens_Import)
SiemensVuln = SiemensVuln.drop_duplicates()
SiemensVuln['Vendor'] = 'siemens ag'
SiemensVuln = SiemensVuln.reset_index()
SiemensVuln = SiemensVuln.drop(columns = 'index')

#pepperl fuchs
fPepperl = PepperlVuln.drop_duplicates()
PepperlVuln = fPepperl

#festo
fFesto = fFesto.drop_duplicates()
FestoVuln = fFesto




#importing iPortal data from excel

#iPortal Data
iPortal_Import = pd.read_excel(file_path + 'allData.xlsx', sheet_name = 'IPortal')
iPortal_Import = iPortal_Import.drop(columns = ['Vendor ID'])
iPortal_Import = iPortal_Import.drop_duplicates()

#cleaning and separating data
IPortalDf = pd.DataFrame(data=iPortal_Import)
IPortalDf['Product name'] = IPortalDf['Product name'].str.lower()
IPortalDf['Online FW version'] = IPortalDf['Online FW version'].str.lower()
IPortalDf['Vendor'] = IPortalDf['Vendor'].str.lower()

#getting siemens iPortal
iPortal_Siemens = iPortal_Import[iPortal_Import['Vendor'].str.contains('siemens')]
iPortal_Siemens = iPortal_Siemens.reset_index()
iPortal_Siemens = iPortal_Siemens.drop(columns = 'index')

#getting sick iPortal
iPortal_Sick = iPortal_Import[iPortal_Import['Vendor'].str.contains('sick')]
iPortal_Sick = iPortal_Sick.reset_index()
iPortal_Sick = iPortal_Sick.drop(columns = 'index')

#getting festo iPortal
iPortal_Festo = iPortal_Import[iPortal_Import['Vendor'].str.contains('festo')]
iPortal_Festo['Vendor'] = 'festo'
iPortal_Festo = iPortal_Festo.reset_index()
iPortal_Festo = iPortal_Festo.drop(columns = 'index')

#getting pepperl iPortal
iPortal_Pepperl = iPortal_Import[iPortal_Import['Vendor'].str.contains('pepperl')]
iPortal_Pepperl['Vendor'] = 'Pepperl Fuchs'
iPortal_Pepperl = iPortal_Pepperl.reset_index()
iPortal_Pepperl = iPortal_Pepperl.drop(columns = 'index')



###################################            MERGING FUNCTION           ###########################


def mergeDfsCheck(iPortal, Vuln):

    Vuln = Vuln.dropna(subset=['Products'])
    Vuln = Vuln.fillna('NaN')
    Vuln = Vuln.reset_index()
    iPortal = iPortal.dropna()
    iPortal = iPortal.fillna('NaN')
    iPortal = iPortal.reset_index()
    i = 0
    data1 = {'Product name': ['0'], 'Firmware version': ['0'], 'Vendor': ['0'], 'CVSS Score': ['0'], 'Vector': ['0'],
             'Description': ['0'], 'Link': ['0']}
    FinalDf = pd.DataFrame(data = data1)
    for index, row in iPortal.iterrows():
        substring = iPortal['Product name'].values[i]
        print(substring)
        i = i + 1
        j = 0
        for index, row in Vuln.iterrows():
            string = Vuln['Products'].values[j]
            if substring in string:
            
                print('j/the index or is:')
                print(j)
                print('the i index is:')
                print(i)
            
                pName = iPortal.at[i, 'Product name']
                FWV = iPortal.at[i, 'Online FW version']
                Vendor = iPortal.at[i, 'Vendor']
                CVSS = Vuln.at[j, 'CVSS Score']
                Vector = Vuln.at[j, 'Vector']
                Products = Vuln.at[j, 'Products']
                Link = Vuln.at[j, 'Link']
            
                data2 = {'Vendor': [Vendor], 'Product name': [pName], 'Firmware version': [FWV], 'CVSS Score': [CVSS], 
                        'Vector': [Vector], 'Description': [Products], 'Link': [Link]}
                result = pd.DataFrame(data = data2)

                print(result)
                FinalDf = FinalDf.append(result) 
                del result
            
            else:
                print('no match')      
            j = j + 1

    rowCount = FinalDf['Product name'].count()        
    if rowCount > 1:
        print('Dataframe has been populated')
        FinalDf = FinalDf[['Vendor', 'Product name', 'Firmware version', 'CVSS Score', 'Vector', 'Description', 'Link']]
        FinalDf = FinalDf.reset_index()
        FinalDf = FinalDf.drop(0)
        FinalDf = FinalDf.reset_index()
        FinalDf = FinalDf.drop(columns = ['index', 'level_0'])
    else:
        print('Dataframe has not been populated')
    return(FinalDf)



################################## RUNNING FUNCTION FOR ALL VENDORS               ###################

#running function on all vendor dataframes
SiemensF = mergeDfsCheck(iPortal_Siemens, SiemensVuln)
FestoF = mergeDfsCheck(iPortal_Festo, FestoVuln)
SickF = mergeDfsCheck(iPortal_Sick, SickVuln)
PepperlF = mergeDfsCheck(iPortal_Pepperl, PepperlVuln)



#######################                  EXPORTING FINAL DFS TO EXCEL            #######################



#exporting all final dataframes of vulnerabilities

file_path = 'C:/Users/matth/Desktop/Coding/WebScrape/'
writer = pd.ExcelWriter(file_path + 'VulnerabilityData.xlsx')
SiemensF.to_excel(writer, 'Siemens Vulnerabilities')
FestoF.to_excel(writer, 'Festo Vulnerabilities')
SickF.to_excel(writer, 'Sick Vulnerabilities')
PepperlF.to_excel(writer, 'Pepperl Fuchs Vulnerabilities')
writer.save()





